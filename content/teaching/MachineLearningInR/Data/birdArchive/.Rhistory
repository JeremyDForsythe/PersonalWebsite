confusionMatrix(predictions,testing$Species)
library(reshape2)
cm <- confusionMatrix (predictions,testing$Species)
cm_df <- melt(cm$table)
ggplot(cm_df, aes(x = Prediction, y = Reference, fill = value)) +
geom_raster() + scale_fill_distiller(palette = "Spectral")
cm_df
indT <- createDataPartition(y=iris$Species,p=0.6,list=FALSE)
training <- iris[indT,]
testing  <- iris[-indT,]
ModFit_KNN <- train(Species~.,training,method="knn",tuneGrid = expand.grid(k = 1:25))
ggplot(ModFit_KNN$results,aes(k,Accuracy))+
geom_point(color="blue")+
labs(title=paste("Optimum K is ",ModFit_KNN$bestTune),
y="Accuracy")
predict_KNN<- predict(ModFit_KNN,newdata=testing)
table(predict_KNN)
confusionMatrix(testing$Species,predict_KNN)
confusionMatrix(testing$Species,predict_KNN)
cm <- confusionMatrix (predict_KNN,testing$Species)
cm_df <- melt(cm$table)
ggplot(cm_df, aes(x = Prediction, y = Reference, fill = value)) +
geom_raster() + scale_fill_distiller(palette = "Spectral")
library(tensorflow) # Load Tensorflow R Package
install_tensorflow(extra_packages="pillow")
library(tensorflow) # Load Tensorflow R Package
install_tensorflow(extra_packages="pillow")
knitr::opts_chunk$set(echo = TRUE)
### Libraries
library(ggplot2) # For Cutting Data Into Equal Bins With Quantile
library(bigleaf) # Functions useful for calculating inputs for the Preistly Taylor ET Estimation
library(sensitivity) # For The Sensitivity Anaylses
library(sensobol) # For The Sensitivity Anaylses
library(lavaan)
library(semPlot)
library(OpenMx)
library(car)
library(mgcv)
library(caret)
library(reticulate)
use_python("/usr/bin/python3")
library(reshape2)
library(lubridate)
### Functions
options(scipen = 9999) #Prevents Scientific Notation & Rounding Errors with The Timestamp Columns
JDF_Colorlist<-c('indianred','cornflowerblue','darkorchid','forestgreen','lightpink2','lightskyblue','lightseagreen','lightsalmon2','sienna', 'yellowgreen','firebrick1','lavenderblush3','olivedrab4','yellow3','plum4','sienna1','slategray','darkblue')
EP_FullOutputReader<-function(Filename){
headers <- read.csv(Filename, skip = 1, header = F, nrows = 1, as.is = T)
eddyFULLout <-  read.csv(Filename, skip = 3, header=FALSE, stringsAsFactors=FALSE)
colnames(eddyFULLout)<- headers
eddyFULLout[eddyFULLout == -7999] <- NA
eddyFULLout[eddyFULLout == -9999] <- NA
eddyFULLout$timeR<-as.POSIXct(as.character(paste(eddyFULLout$date,eddyFULLout$time, sep = " ")),format = "%Y-%m-%d %H:%M", tz = "EST")
return(eddyFULLout)
}
REddyReader<-function(path2REddyProcOutputTXT){
headers<-read.delim(path2REddyProcOutputTXT, nrows = 1, header = FALSE)
headers<-as.character(headers[1,])
headers<-gsub(" ", "", headers, fixed = TRUE)
outDF <- read.delim(path2REddyProcOutputTXT, na.strings="-9999", skip = 2, header=FALSE)
colnames(outDF)<-headers
outDF$DateTime<-as.POSIXct(outDF$DateTime,format = "%Y-%m-%d %H:%M:%S", tz = "EST")
outDF[outDF == -9999] <- NA #sometimes R misses these on the import. This is just a double check.
return(outDF)
}
afReader<-function(path2AmerifluxCSV){
outDF<-read.csv(path2AmerifluxCSV, na.strings="-9999")
outDF[outDF == -9999] <- NA #Sometimes R misses these on the import. This is just a double check.
outDF$TIMESTAMP_START<-as.POSIXct(as.character(outDF$TIMESTAMP_START), tz="EST",format = "%Y%m%d%H%M" )
outDF$TIMESTAMP_END<-as.POSIXct(as.character(outDF$TIMESTAMP_END), tz="EST",format = "%Y%m%d%H%M" )
return(outDF)
}
gfReader<-function(path2gfCSV){
# Reads the gapfilled and partitioned data output from Jeremy's Machine Learning Code Located in REddProcExperiments Folder
outDF<-read.csv(path2gfCSV)
outDF$TIMESTAMP_START<- as.POSIXct(outDF$TIMESTAMP_START, tz = 'EST')
outDF$TIMESTAMP_END<- as.POSIXct(outDF$TIMESTAMP_END, tz = 'EST')
return(outDF)
}
rbindNfill<-function(dfList){
# Custom Function to rbind a list of dataframes with differing num of columns.
# Returns a dataframe with NAs for columns that didn't exist in the dfs.
out<-do.call(rbind,
c(lapply(dfList,
function(x) data.frame(c(x, sapply(setdiff(unique(unlist(lapply(dfList, names))), names(x)),
function(y) NA)))),
make.row.names=FALSE))
rownames(out)<-NULL
return(out)
}
R2calc<-function(Model,Observation){
if (length(Model) != length(Observation)){
stop('Model & Observations have differing lengths.')
}
meanModel<-mean(Model)
meanTower<-mean(Observation)
numerator<-vector()
denomTower<-vector()
denomModel<-vector()
for (i in 1:length(Model)){
numerator<-c(numerator,(Observation[i]-meanTower)*(Model[i]-meanModel))
denomTower<-c(denomTower,(Observation[i] - meanTower)^2)
denomModel<-c(denomModel,(Model[i] - meanModel)^2)
}
numerator<-sum(numerator)
denominator<-sqrt(sum(denomTower))*sqrt(sum(denomModel))
out<-(numerator/denominator)^2
return(out)
}
plotScatterColDens<-function(x,y,numBins = 128, ...){
df<-data.frame(x,y)
## Use densCols() output to get density at each point
temp <- densCols(x,y, colramp=colorRampPalette(c("black", "white")), nbin = numBins)
df$dens <- col2rgb(temp)[1,] + 1L
## Map densities to colors
cols <-  colorRampPalette(c("#000099", "#00FEFF", "#45FE4F",
"#FCFF00", "#FF9400", "#FF3100"))(256)
df$col <- cols[df$dens]
## Plot it, reordering rows so that densest points are plotted on top
plot(y~x, data=df[order(df$dens),], col=col, ...)
}
ptETcalc<-function(AirTemperature,Pressure,SoilHeatFlux,DiffuseFraction,ShortwaveIn,ShortwaveOut,LongwaveIn,LongwaveOut){
#Reference https://wetlandscapes.github.io/blog/blog/penman-monteith-and-priestley-taylor/
Es<-.611*exp((17.2694*AirTemperature)/(AirTemperature + 237.3)) #Saturation vaporpressure in kPa (Tair in oC)
Delta<-(4098*Es)/((AirTemperature+237.3)^2)
lambdaV<-latent.heat.vaporization(AirTemperature)
gamma<-psychrometric.constant(Tair = AirTemperature, pressure = Pressure)
NetRadiation<-((DiffuseFraction * ShortwaveIn) + ((1-DiffuseFraction) * ShortwaveIn) - ShortwaveOut) + (LongwaveIn - LongwaveOut)
ETpt<-1.26*(Delta * (NetRadiation - SoilHeatFlux)) / (lambdaV * (Delta + gamma))*1000*2
return(ETpt)
}
ptETcalcComponents<-function(AirTemperature,Pressure,SoilHeatFlux,ShortwaveDirect,ShortwaveDiffuse,ShortwaveOut,LongwaveIn,LongwaveOut){
#Reference https://wetlandscapes.github.io/blog/blog/penman-monteith-and-priestley-taylor/
Es<-.611*exp((17.2694*AirTemperature)/(AirTemperature + 237.3)) #Saturation vaporpressure in kPa (Tair in oC)
Delta<-(4098*Es)/((AirTemperature+237.3)^2)
lambdaV<-latent.heat.vaporization(AirTemperature)
gamma<-psychrometric.constant(Tair = AirTemperature, pressure = Pressure)
NetRadiation<-((ShortwaveDiffuse + ShortwaveDirect) - ShortwaveOut) + (LongwaveIn - LongwaveOut)
ETpt<-1.26*(Delta * (NetRadiation - SoilHeatFlux)) / (lambdaV * (Delta + gamma))*1000*2
return(ETpt)
}
sentinelReader<-function(path2SentinelOutput,SCL_Filter=c(0.0004)){
############ Data Loading
sentinelBands <- read.csv(path2SentinelOutput) # See https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR#bands For Column Descriptions
sentinelBands<-subset(sentinelBands,SCL %in% SCL_Filter) #Quality Filter, See https://sentinels.copernicus.eu/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm for more information
timeR<-sapply(sentinelBands$date, FUN = timeExtractor, USE.NAMES = FALSE)
timeR<-as.POSIXct(timeR, tz="UTC",format = "%Y%m%dT%H%M%S" )
latlong<-cordExtractor(sentinelBands$.geo)
sentinelBands<-cbind(timeR,latlong,sentinelBands)
sentinelBands<-colDropper(df=sentinelBands,cols2dropVec = c("system.index","ID","SCL","date",".geo","QA10","QA20","QA60"))
############ Calculate VIs
#249 Total options can found at https://www.indexdatabase.de/db/is.php?sensor_id=96
sentinelBands$SR<-sentinelBands$B8/sentinelBands$B4
sentinelBands$NDVI<-(sentinelBands$B8-sentinelBands$B4)/(sentinelBands$B8+sentinelBands$B4)
sentinelBands$SAVI<-(sentinelBands$B8-sentinelBands$B4)/(sentinelBands$B8 + sentinelBands$B4 + 0.428) * (1.0 + 0.428)
sentinelBands$EVI<-(sentinelBands$B8 -sentinelBands$B4) / ((sentinelBands$B8 + 6.0 * sentinelBands$B4-7.5 * sentinelBands$B2) + 1.0)
sentinelBands$EVI2<-2.4 * (sentinelBands$B8 - sentinelBands$B4) / (sentinelBands$B8 + sentinelBands$B4 + 1.0)
sentinelBands$NDMI<- (sentinelBands$B8 - sentinelBands$B11) / (sentinelBands$B8 + sentinelBands$B11)
sentinelBands$NIRv<- ((sentinelBands$B8-sentinelBands$B4)/(sentinelBands$B8+sentinelBands$B4)) * sentinelBands$B8
sentinelBands$CIr<-(sentinelBands$B7/sentinelBands$B5)-1
############ Return Output
return(sentinelBands)
}
#Robust Z-score Method For Removing Outliers
robustZoutliers<-function(x,print=FALSE){
# get median
med = median(x)
# subtract median from each value of x and get absolute deviation
abs_dev = abs(x-med)
# get MAD
mad = 1.4826 * median(abs_dev)
# get threshold values for outliers
Tmin = med-(3*mad)
Tmax = med+(3*mad)
# find outliers
if (print==TRUE){
print(paste(c('Outliers:',x[which(x < Tmin | x > Tmax)])))
}
# NA outliers
x[which(x < Tmin | x > Tmax)]<-NA
return(x)
}
timeExtractor<-function(dateVal){
return(unlist(strsplit(dateVal,'_'))[[1]])
}
cordExtractor<-function(geoVec){
lat<-vector()
long<-vector()
pb <- txtProgressBar(min = 0, max = length(geoVec), style = 3)
for (i in 1:length(geoVec)){
focus<-geoVec[i]
split1<-unlist(strsplit(focus, "\\["))[2]
split2<-unlist(strsplit(split1, "\\]"))[1]
split3<-unlist(strsplit(split2, ","))
lat<-c(lat,as.numeric(split3[[1]]))
long<-c(long,as.numeric(split3[[2]]))
setTxtProgressBar(pb, i)
}
return(data.frame(lat,long))
}
colDropper<-function(df,cols2dropVec){
out<-df[ , !(names(df) %in% cols2dropVec)]
return(out)
}
percentDiff<-function(numVec1,numVec2){
if(length(numVec1)!=length(numVec2)){
stop('The Numeric Vectors Have Different Lengths')
}
absDif<-abs(numVec1-numVec2)
halfSum<-(numVec1+numVec2)/2
difPercent<-absDif/halfSum*100
return(difPercent)
}
umolCO2_to_gC <- function(CO2_flux){
#Converts CO2 quantities from umol CO2 m-2 s-1 to  g C m-2 s-1
#1e-06 conversion micromole (umol) to mole (mol)
#0.012011 molar mass of carbon (kg mol-1)
#1000 conversion kilogram (kg) to gram (g)
#86400 seconds per day
out <- CO2_flux * 1e-06 * 0.012011 * 1000
return(out)
}
gapfillKNNimputation<-function(ALLnumDF){
PreImputeKNN <- preProcess(ALLnumDF,method="knnImpute",k=5)
DataImputeKNN <- predict(PreImputeKNN, ALLnumDF) # Note: Standardizes the Data
Rescale<- as.data.frame(t(t(DataImputeKNN)*PreImputeKNN$std+PreImputeKNN$mean)) #Convert back to original scale
return(Rescale)
}
knitr::opts_chunk$set(echo = TRUE)
############ Library Loading
library(caret)
library(ggplot2)
library(GGally)
library(rattle)
library(adabag)
library(PerformanceAnalytics)
library(glmnet)
library(plotmo)
library(neuralnet)
library(e1071)
library(purrr)
library(tidyr)
library(naniar)
library(UpSetR)
library(RANN)
library(pls)
library(randomForest)
library(gbm)
library(kernelshap)
library(shapviz)
library(kernlab)
library(xtable)
options(xtable.comment = FALSE)
set.seed(12) # Forces our random numbers to be reproducible
flux<-read.csv("https://raw.githubusercontent.com/JeremyDForsythe/PersonalWebsite/master/content/teaching/MachineLearningInR/US-SB3_2023_Example.csv")
colnames(flux)
target<-"NEE" # Could be multiple targets
predictors<-c("SW_IN","VPD","TA","SWC")
flux$TIMESTAMP_END<-as.POSIXct(as.character(flux$TIMESTAMP_END),
tz="EST",
format = "%Y-%m-%d %H:%M:%S")
### Impute To Gapfill NAs in Predictor Datasets with K-Nearest Neighbor Imputation
PreImputeKNN <- preProcess(flux[predictors],method="knnImpute",k=5)
DataImputeKNN <- predict(PreImputeKNN, flux[predictors]) # Note: Standardizes the Data
Rescale<- as.data.frame(t(t(DataImputeKNN)*PreImputeKNN$std+PreImputeKNN$mean)) #Convert back to original scale
ImputeFlux<-cbind(flux[target],Rescale)
rm(PreImputeKNN,DataImputeKNN,Rescale)
vis_miss(ImputeFlux, warn_large_data = FALSE)
### For Model Training Remove NAs in Target
Impute<-ImputeFlux[complete.cases(ImputeFlux), ]
partitionIndex <- createDataPartition(y=Impute$NEE,p=0.6,list=FALSE,times=1) #Create the index of 'keeps'
training <- Impute[partitionIndex,] # Convert 'keeps' to Training
testing  <- Impute[-partitionIndex,] # The Rest to Testing
fitControl <- trainControl(method="cv", number=10, allowParallel = TRUE) #in this case k=10
library(tensorflow) # Load Tensorflow R Package
install_tensorflow(extra_packages="pillow")
use_python("/usr/bin/python3")
library(tensorflow) # Load Tensorflow R Package
install_tensorflow(extra_packages="pillow")
#use_python("/usr/bin/python3") Just For Linux
install_tensorflow(extra_packages="pillow")
knitr::opts_chunk$set(echo = TRUE)
library(keras)
install_keras()
knitr::opts_chunk$set(echo = TRUE)
setwd("/home/jeremyforsythe/Documents/NAU/Archive/Teaching Machine Learning/BirdImageData/birdArchive/")
label_list <- dir("train/")
output_n <- length(label_list)
label_list <- dir("train/")
label_list <- dir("./train/")
label_list <-list.files('/train')
setwd("/home/jeremyforsythe/Documents/NAU/Archive/Teaching Machine Learning/BirdImageData/birdArchive/")
label_list <-list.files('/train')
label_list <- dir("train/")
label_list <- dir("./train/")
label_list <- dirs("./train/")
label_list <- list.files("train/", recursive = TRUE)
label_list <- list.files("/home/jeremyforsythe/Documents/NAU/Archive/Teaching Machine Learning/BirdImageData/birdArchive/train/")
setwd("/home/jeremyforsythe/Documents/NAU/Archive/Teaching Machine Learning/BirdImageData/birdArchive/")
label_list <- list.files("/home/jeremyforsythe/Documents/NAU/Archive/Teaching Machine Learning/BirdImageData/birdArchive/train/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
width <- 224
height<- 224
label_list <- list.files("~/train/")
setwd("/home/jeremyforsythe/Documents/NAU/Archive/Teaching Machine Learning/BirdImageData/birdArchive/")
label_list <- list.files("/home/jeremyforsythe/Documents/NAU/Archive/Teaching Machine Learning/BirdImageData/birdArchive/train/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
width <- 224
height<- 224
target_size <- c(width, height)
rgb <- 3 #color channels
path_train <- "/train/"
train_data_gen <- image_data_generator(rescale = 1/255,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = label_list,
seed = 2021)
path_train <- "/home/jeremyforsythe/Documents/NAU/Archive/Teaching Machine Learning/BirdImageData/birdArchive/train/"
train_data_gen <- image_data_generator(rescale = 1/255,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = label_list,
seed = 2021)
validation_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'validation',
target_size = target_size,
class_mode = "categorical",
classes = label_list,
seed = 2021)
table(train_images$classes)
plot(as.raster(train_images[[1]][[1]][17,,,]))
plot(as.raster(train_images[[1]][[1]][18,,,]))
plot(as.raster(train_images[[1]][[1]][1,,,]))
plot(as.raster(train_images[[1]][[1]][122,,,]))
plot(as.raster(train_images[[1]][[1]][22,,,]))
plot(as.raster(train_images[[2]][[1]][22,,,]))
plot(as.raster(train_images[[2]][[6]][22,,,]))
mod_base <- application_xception(weights = 'imagenet',
include_top = FALSE, input_shape = c(width, height, 3))
freeze_weights(mod_base)
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(lr = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model
hist <- model %>% fit_generator(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
batch_size <- 32
epochs <- 6
hist <- model %>% fit_generator(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
setwd("/home/jeremyforsythe/Documents/NAU/Archive/Teaching Machine Learning/BirdImageData/birdArchive/")
label_list <- list.files("/home/jeremyforsythe/Documents/NAU/Archive/Teaching Machine Learning/BirdImageData/birdArchive/train/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
width <- 224
height<- 224
target_size <- c(width, height)
rgb <- 3 #color channels
path_train <- "/home/jeremyforsythe/Documents/NAU/Archive/Teaching Machine Learning/BirdImageData/birdArchive/train/"
train_data_gen <- image_data_generator(rescale = 1/255,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = label_list,
seed = 2021)
validation_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'validation',
target_size = target_size,
class_mode = "categorical",
classes = label_list,
seed = 2021)
plot(as.raster(train_images[[1]][[1]][18,,,]))
mod_base <- application_xception(weights = 'imagenet',
include_top = FALSE, input_shape = c(width, height, 3))
freeze_weights(mod_base)
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model
batch_size <- 32
epochs <- 6
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
batch_size <- 16
epochs <- 6
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
batch_size <- 32
epochs <- 3
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
reticulate::py_last_error()
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
